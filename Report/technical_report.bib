%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jagmohan Meher at 2022-06-22 20:34:40 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{Sochat2017,
	abstract = {Here we present Singularity Hub, a framework to build and deploy Singularity containers for mobility of compute, and the singularity-python software with novel metrics for assessing reproducibility of such containers. Singularity containers make it possible for scientists and developers to package reproducible software, and Singularity Hub adds automation to this workflow by building, capturing metadata for, visualizing, and serving containers programmatically. Our novel metrics, based on custom filters of content hashes of container contents, allow for comparison of an entire container, including operating system, custom software, and metadata. First we will review Singularity Hub's primary use cases and how the infrastructure has been designed to support modern, common workflows. Next, we conduct three analyses to demonstrate build consistency, reproducibility metric and performance and interpretability, and potential for discovery. This is the first effort to demonstrate a rigorous assessment of measurable similarity between containers and operating systems. We provide these capabilities within Singularity Hub, as well as the source software singularity-python that provides the underlying functionality. Singularity Hub is available at https://singularity-hub.org, and we are excited to provide it as an openly available platform for building, and deploying scientific containers.},
	author = {Sochat, Vanessa V. AND Prybol, Cameron J. AND Kurtzer, Gregory M.},
	date-added = {2022-05-26 11:04:39 +0800},
	date-modified = {2022-05-26 11:04:51 +0800},
	doi = {10.1371/journal.pone.0188511},
	journal = {PLOS ONE},
	month = {11},
	number = {11},
	pages = {1-24},
	publisher = {Public Library of Science},
	title = {Enhancing reproducibility in scientific computing: Metrics and registry for Singularity containers},
	url = {https://doi.org/10.1371/journal.pone.0188511},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0188511}}

@article{MDAR2021,
	author = {Malcolm Macleod and Andrew M. Collings and Chris Graf and Veronique Kiermer and David Mellor and Sowmya Swaminathan and Deborah Sweet and Valda Vinson},
	date-added = {2022-04-29 18:12:21 +0800},
	date-modified = {2022-04-29 18:12:35 +0800},
	doi = {10.1073/pnas.2103238118},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2103238118},
	journal = {Proceedings of the National Academy of Sciences},
	number = {17},
	pages = {e2103238118},
	title = {The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2103238118},
	volume = {118},
	year = {2021},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.2103238118},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2103238118}}

@webpage{Sciscore,
	date-added = {2022-04-29 17:45:26 +0800},
	date-modified = {2022-04-29 17:45:58 +0800},
	lastchecked = {29 Apr 2022},
	url = {https://www.sciscore.com/},
	bdsk-url-1 = {https://www.sciscore.com/}}

@booklet{SCORE,
	author = {Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel and Benkler, Noam and Bishop, Michael and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Dreber, Anna and Errington, Timothy and Fidler, Fiona and Fox, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Wu, Jian},
	date-added = {2022-04-29 17:44:03 +0800},
	date-modified = {2022-04-29 17:47:43 +0800},
	doi = {10.31235/osf.io/46mnb},
	month = {05},
	title = {Systematizing Confidence in Open Research and Evidence (SCORE)},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.31235/osf.io/46mnb}}

@article{Altmejd2019,
	author = {Altmejd, Adam and Dreber, Anna and Forsell, Eskil and Huber, J{\"u}rgen and Imai, Taisuke and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Camerer, Colin},
	date-added = {2022-04-29 17:39:48 +0800},
	date-modified = {2022-04-29 17:40:09 +0800},
	doi = {10.1371/journal.pone.0225826},
	journal = {PLOS ONE},
	month = {12},
	pages = {e0225826},
	title = {Predicting the replicability of social science lab experiments},
	volume = {14},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0225826}}

@article{Vijaykumar2019,
	abstract = {Machine learning methods have become very popular in diverse fields due
   to their focus on predictive accuracy, but little work has been
   conducted on how to assess the replicability of their findings. We
   introduce and adapt replication methods advocated in psychology to the
   aims and procedural needs of machine learning research. In Study 1, we
   illustrate these methods with the use of an empirical data set,
   assessing the replication success of a predictive accuracy measure,
   namely, R-2 on the cross-validated and test sets of the samples. We
   introduce three replication aims. First, tests of inconsistency examine
   whether single replications have successfully rejected the original
   study. Rejection will be supported if the 95\% confidence interval (CI)
   of R-2 difference estimates between replication and original does not
   contain zero. Second, tests of consistency help support claims of
   successful replication. We can decide apriori on a region of
   equivalence, where population values of the difference estimates are
   considered equivalent for substantive reasons. The 90\% CI of a
   different estimate lying fully within this region supports replication.
   Third, we show how to combine replications to construct meta-analytic
   intervals for better precision of predictive accuracy measures. In Study
   2, R-2 is reduced from the original in a subset of replication studies
   to examine the ability of the replication procedures to distinguish true
   replications from nonreplications. We find that when combining studies
   sampled from same population to form meta-analytic intervals,
   random-effects methods perform best for cross-validated measures while
   fixed-effects methods work best for test measures. Among machine
   learning methods, regression was comparable to many complex methods,
   while support vector machine performed most reliably across a variety of
   scenarios. Social scientists who use machine learning to model empirical
   data can use these methods to enhance the reliability of their findings.},
	address = {2455 TELLER RD, THOUSAND OAKS, CA 91320 USA},
	affiliation = {Vijayakumar, R (Corresponding Author), Natl Univ Singapore, Dept Psychol, Fac Arts \& Social Sci, Block AS4,Level 2,9 Arts Link, Singapore 117570, Singapore. Vijayakumar, Ranjith; Cheung, Mike W-L, Natl Univ Singapore, Dept Psychol, Singapore, Singapore.},
	article-number = {0894439319888445},
	author = {Vijayakumar, Ranjith and Cheung, Mike W-L},
	author-email = {r.v@u.nus.edu},
	da = {2022-04-28},
	date-added = {2022-04-28 12:07:51 +0800},
	date-modified = {2022-04-28 12:08:00 +0800},
	doc-delivery-number = {UL7BU},
	doi = {10.1177/0894439319888445},
	earlyaccessdate = {DEC 2019},
	eissn = {1552-8286},
	funding-acknowledgement = {Ministry of Education, SingaporeMinistry of Education, Singapore {[}FY2017-FRC1-008]},
	funding-text = {The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Mike W.-L. Cheung was supported by the Academic Research Fund Tier 1 (FY2017-FRC1-008) from the Ministry of Education, Singapore.},
	issn = {0894-4393},
	journal = {SOCIAL SCIENCE COMPUTER REVIEW},
	journal-iso = {Soc. Sci. Comput. Rev.},
	keywords = {machine learning; model comparison; predictive accuracy; psychological research; replicability},
	keywords-plus = {CONFIDENCE-INTERVALS; CLASSIFICATION MODELS; BIAS; REPLICATION; REGRESSION; CRISIS; ERROR; POWER; RISK; TIME},
	language = {English},
	month = {OCT},
	number = {5, SI},
	number-of-cited-references = {62},
	orcid-numbers = {Cheung, Mike W.-L./0000-0003-0113-0758},
	pages = {768-801},
	publisher = {SAGE PUBLICATIONS INC},
	research-areas = {Computer Science; Information Science \& Library Science; Social Sciences - Other Topics},
	researcherid-numbers = {Cheung, Mike W.-L./B-9914-2008},
	times-cited = {1},
	title = {Assessing Replicability of Machine Learning Results: An Introduction to Methods on Predictive Accuracy in Social Sciences},
	type = {Article},
	unique-id = {WOS:000501954800001},
	usage-count-last-180-days = {2},
	usage-count-since-2013 = {11},
	volume = {39},
	web-of-science-categories = {Computer Science, Interdisciplinary Applications; Information Science \& Library Science; Social Sciences, Interdisciplinary},
	web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1177/0894439319888445}}

@article{Brnabic2021,
	abstract = {Machine learning is a broad term encompassing a number of methods that allow the investigator to learn from the data. These methods may permit large real-world databases to be more rapidly translated to applications to inform patient-provider decision making.},
	author = {Brnabic, Alan and Hess, Lisa M.},
	date = {2021/02/15},
	date-added = {2022-04-28 11:19:59 +0800},
	date-modified = {2022-04-28 11:20:11 +0800},
	doi = {10.1186/s12911-021-01403-2},
	id = {Brnabic2021},
	isbn = {1472-6947},
	journal = {BMC Medical Informatics and Decision Making},
	number = {1},
	pages = {54},
	title = {Systematic literature review of machine learning methods used in the analysis of real-world data for patient-provider decision making},
	url = {https://doi.org/10.1186/s12911-021-01403-2},
	volume = {21},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1186/s12911-021-01403-2}}

@article{Carvalho2019,
	abstract = {The amount of data extracted from production processes has increased exponentially due to the proliferation of sensing technologies. When processed and analyzed, data can bring out valuable information and knowledge from manufacturing process, production system and equipment. In industries, equipment maintenance is an important key, and affects the operation time of equipment and its efficiency. Thus, equipment faults need to be identified and solved, avoiding shutdown in the production processes. Machine Learning (ML) methods have been emerged as a promising tool in Predictive Maintenance (PdM) applications to prevent failures in equipment that make up the production lines in the factory floor. However, the performance of PdM applications depends on the appropriate choice of the ML method. The aim of this paper is to present a systematic literature review of ML methods applied to PdM, showing which are being explored in this field and the performance of the current state-of-the-art ML techniques. This review focuses on two scientific databases and provides a useful foundation on the ML techniques, their main results, challenges and opportunities, as well as it supports new research works in the PdM field.},
	author = {Thyago P. Carvalho and Fabr{\'\i}zzio A. A. M. N. Soares and Roberto Vita and Roberto da P. Francisco and Jo{\~a}o P. Basto and Symone G. S. Alcal{\'a}},
	date-added = {2022-04-28 11:18:15 +0800},
	date-modified = {2022-04-28 11:18:45 +0800},
	doi = {https://doi.org/10.1016/j.cie.2019.106024},
	issn = {0360-8352},
	journal = {Computers & Industrial Engineering},
	keywords = {Predictive maintenance, Machine learning, PdM, Systematic literature review, Artificial intelligence},
	pages = {106024},
	title = {A systematic literature review of machine learning methods applied to predictive maintenance},
	url = {https://www.sciencedirect.com/science/article/pii/S0360835219304838},
	volume = {137},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0360835219304838},
	bdsk-url-2 = {https://doi.org/10.1016/j.cie.2019.106024}}

@article{Banati2016,
	abstract = {Scientific workflows are efficient tools for specifying and automating
   compute and data intensive in-silico experiments. An important challenge
   related to their usage is their reproducibility. In order to make it
   reproducible, many factors have to be investigated which can influence
   and even prevent this process: the missing descriptions and samples; the
   missing provenance data about the environmental parameters and the data
   dependencies; the dependencies of executions which are based on special
   hardware, changing or volatile third party services or random generated
   values. Some of these factors (called dependencies) can be eliminated by
   careful design or by huge resource usage but most of them cannot be
   bypassed. Our investigation deals with the critical dependencies of
   execution. In this paper we set up a mathematical model to evaluate the
   results of the workflow in addition we provide a mechanism to make the
   workflow reproducible based on provenance data and statistical tools.},
	address = {BECSI UT 96-B, BUDAPEST, H-1034, HUNGARY},
	affiliation = {Banati, A (Corresponding Author), Obuda Univ, John von Neumann Fac Informat, Becsi Ut 96-B, H-1034 Budapest, Hungary. Kacsuk, Peter; Kozlovszky, Miklos, MTA SZTAKI, Pf 63, H-1518 Budapest, Hungary. Kacsuk, Peter, Univ Westminster, 115 New Cavendish St, London W1W 6UW, England. Banati, Anna; Kozlovszky, Miklos, Obuda Univ, John von Neumann Fac Informat, Becsi Ut 96-B, H-1034 Budapest, Hungary.},
	author = {Banati, Anna and Kacsuk, Peter and Kozlovszky, Miklos},
	author-email = {banati.anna@nik.uni-obuda.hu peter.kacsuk@sztaki.mta.hu kozlovszky.miklos@nik.uni-obuda.hu},
	da = {2022-04-27},
	date-added = {2022-04-27 18:03:00 +0800},
	date-modified = {2022-04-27 18:03:16 +0800},
	doc-delivery-number = {EW4MT},
	doi = {10.12700/APH.14.2.2017.2.11},
	funding-acknowledgement = {EUEuropean Commission},
	funding-text = {This work was supported by EU project SCI-BUS (SCIentific gateway Based User Support). The SCI-BUS project aims to ease the life of the e-Scientists by creating a new science gateway customization methodology based on the generic-purpose gUSE/WS-PGRADE portal family.},
	issn = {1785-8860},
	journal = {ACTA POLYTECHNICA HUNGARICA},
	journal-iso = {Acta Polytech. Hung.},
	keywords = {scientific workflows; reproducibility; analytical model; provenance; evaluation; gUSE},
	language = {English},
	number = {2},
	number-of-cited-references = {23},
	oa = {Green Accepted, Bronze},
	orcid-numbers = {Kozlovszky, Miklos/0000-0001-8096-9628},
	pages = {201-217},
	publisher = {BUDAPEST TECH},
	research-areas = {Engineering},
	researcherid-numbers = {Kozlovszky, Miklos/AAM-1437-2021},
	times-cited = {3},
	title = {Reproducibility Analysis of Scientific Workflows},
	type = {Article},
	unique-id = {WOS:000402476300011},
	usage-count-last-180-days = {3},
	usage-count-since-2013 = {4},
	volume = {14},
	web-of-science-categories = {Engineering, Multidisciplinary},
	web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.12700/APH.14.2.2017.2.11}}

@article{Yang2020,
	abstract = {After years of urgent concern about the failure of scientific papers to replicate, an accurate, scalable method for identifying findings at risk has yet to arrive. We present a method that combines machine intelligence and human acumen for estimating a study's likelihood of replication. Our model---trained and tested on hundreds of manually replicated studies and out-of-sample datasets ---is comparable to the best current methods, yet reduces the strain on researchers' resources. In practice, our model can complement prediction market and survey replication methods, prioritize studies for expensive manual replication tests, and furnish independent feedback to researchers prior to submitting a study for review. Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study's replicability. Here, we trained an artificial intelligence model to estimate a paper's replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model's generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model's predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like ``remarkable'' or ``unexpected.'' We did find that the model's accuracy is higher when trained on a paper's text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications---a task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
	author = {Yang Yang and Wu Youyou and Brian Uzzi},
	date-added = {2022-04-27 17:57:56 +0800},
	date-modified = {2022-04-27 17:58:09 +0800},
	doi = {10.1073/pnas.1909046117},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1909046117},
	journal = {Proceedings of the National Academy of Sciences},
	number = {20},
	pages = {10762-10768},
	title = {Estimating the deep replicability of scientific findings using human and artificial intelligence},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1909046117},
	volume = {117},
	year = {2020},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vRG93bmxvYWRzL2NpdGF0aW9uLTM1MTMzMjM2My5iaWJPEQF2AAAAAAF2AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8WY2l0YXRpb24tMzUxMzMyMzYzLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAACURvd25sb2FkcwAAAgA2LzpVc2VyczpqYWdtb2hhbm1laGVyOkRvd25sb2FkczpjaXRhdGlvbi0zNTEzMzIzNjMuYmliAA4ALgAWAGMAaQB0AGEAdABpAG8AbgAtADMANQAxADMAMwAyADMANgAzAC4AYgBpAGIADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgA0VXNlcnMvamFnbW9oYW5tZWhlci9Eb3dubG9hZHMvY2l0YXRpb24tMzUxMzMyMzYzLmJpYgATAAEvAAAVAAIAFP//AAAACAANABoAJABNAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAcc=},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1909046117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1909046117}}

@article{Baker2016,
	abstract = {Survey sheds light on the `crisis'rocking research.},
	author = {Baker, Monya},
	date = {2016/05/01},
	date-added = {2022-04-27 17:50:47 +0800},
	date-modified = {2022-04-27 17:50:59 +0800},
	doi = {10.1038/533452a},
	id = {Baker2016},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7604},
	pages = {452--454},
	title = {1,500 scientists lift the lid on reproducibility},
	url = {https://doi.org/10.1038/533452a},
	volume = {533},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/533452a}}

@article{Dreber2015,
	abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants' individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9%) and that a "statistically significant" finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
	author = {Anna Dreber and Thomas Pfeiffer and Johan Almenberg and Siri Isaksson and Brad Wilson and Yiling Chen and Brian A. Nosek and Magnus Johannesson and Kenneth W. Wachter},
	doi = {10.1073/pnas.1516179112},
	issn = {10916490},
	issue = {50},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Prediction markets,Replications,Reproducibility},
	month = {12},
	pages = {15343-15347},
	pmid = {26553988},
	publisher = {National Academy of Sciences},
	title = {Using prediction markets to estimate the reproducibility of scientific research},
	volume = {112},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.1516179112}}

@article{Raghupathi2022,
	abstract = {In computing, research findings are often anecdotally faulted for not being reproducible. Numerous empirical studies have analyzed the reproducibility of a variety of research. Our objective, in this study, is to quantify the current state of reproducibility of research in computing based on prior research, using three reproducibility factors&#x2014;Method, Data and Experiment&#x2014;to measure three different degrees of reproducibility. Twenty-five variables traditionally utilized to document reproducibility are identified and grouped into three factors, namely Method, Data and Experiment. These variables describe the extent to which these factors are documented for each paper. Approximately 100 randomly selected research papers from the International Conference on Information Systems series, for the year 2019, are surveyed. Our findings suggest that none of the papers documented all the variables. In fact, the results show that relatively few variables for each factor are documented. Some of the variables vary across different categories of papers, and most papers fail in at least one of the factors. Reproducibility scores decrease with increased documentation requirements. Reproducibility may improve over time, as researchers prioritize reproducibility and utilize methods that ensure reproducibility. Research documentation in computing is remarkably limited, resulting in a dearth of reproducible factors. Future research may study the shifts and trends in reproducibility over time. Meanwhile, researchers and publishers must increase their focus on the reproducibility aspects of their papers. This study contributes to our understanding of the status quo of reproducibility in computing research.},
	author = {Wullianallur, Raghupathi and Viju, Raghupathi and Jie, Ren},
	doi = {10.1109/ACCESS.2022.3158675},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Business,Codes,Computational modeling,Documentation,Information systems,Reproducibility,Reproducibility of results,Testing,computing,data,experiment,method},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {Reproducibility in Computing Research: An Empirical Study},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2022.3158675}}

@book{national2019reproducibility,
	date-modified = {2022-04-29 17:58:20 +0800},
	editor = {National Academies Press},
	publisher = {National Academies Press},
	title = {Reproducibility and replicability in science},
	year = {2019},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vRG93bmxvYWRzL2NpdGF0aW9uLTM1MDgzNDc4MS5iaWJPEQF2AAAAAAF2AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8WY2l0YXRpb24tMzUwODM0NzgxLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAACURvd25sb2FkcwAAAgA2LzpVc2VyczpqYWdtb2hhbm1laGVyOkRvd25sb2FkczpjaXRhdGlvbi0zNTA4MzQ3ODEuYmliAA4ALgAWAGMAaQB0AGEAdABpAG8AbgAtADMANQAwADgAMwA0ADcAOAAxAC4AYgBpAGIADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgA0VXNlcnMvamFnbW9oYW5tZWhlci9Eb3dubG9hZHMvY2l0YXRpb24tMzUwODM0NzgxLmJpYgATAAEvAAAVAAIAFP//AAAACAANABoAJABNAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAcc=}}

@article{iqbal2016reproducible,
	author = {Iqbal, Shareen A and Wallach, Joshua D and Khoury, Muin J and Schully, Sheri D and Ioannidis, John PA},
	journal = {PLoS biology},
	number = {1},
	pages = {e1002333},
	publisher = {Public Library of Science San Francisco, CA USA},
	title = {Reproducible research practices and transparency across the biomedical literature},
	volume = {14},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAmLi4vLi4vRG93bmxvYWRzL2NpdGF0aW9uLTM1MDgzNDc4MS5iaWJPEQF2AAAAAAF2AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8WY2l0YXRpb24tMzUwODM0NzgxLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAACAAIAAAogY3UAAAAAAAAAAAAAAAAACURvd25sb2FkcwAAAgA2LzpVc2VyczpqYWdtb2hhbm1laGVyOkRvd25sb2FkczpjaXRhdGlvbi0zNTA4MzQ3ODEuYmliAA4ALgAWAGMAaQB0AGEAdABpAG8AbgAtADMANQAwADgAMwA0ADcAOAAxAC4AYgBpAGIADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgA0VXNlcnMvamFnbW9oYW5tZWhlci9Eb3dubG9hZHMvY2l0YXRpb24tMzUwODM0NzgxLmJpYgATAAEvAAAVAAIAFP//AAAACAANABoAJABNAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAcc=}}

@article{Stagge2019Assessing,
	author = {Stagge, James H and Rosenberg, David E and Abdallah, Adel M and Akbar, Hadia and Attallah, Nour A and James, Ryan},
	journal = {Scientific data},
	number = {1},
	pages = {1--12},
	publisher = {Nature Publishing Group},
	title = {Assessing data availability and research reproducibility in hydrology and water resources},
	volume = {6},
	year = {2019}}

@article{stodden2018empirical,
	author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
	journal = {Proceedings of the National Academy of Sciences},
	number = {11},
	pages = {2584--2589},
	publisher = {National Acad Sciences},
	title = {An empirical analysis of journal policy effectiveness for computational reproducibility},
	volume = {115},
	year = {2018}}

@article{polanin2020transparency,
	author = {Polanin, Joshua R and Hennessy, Emily A and Tsuji, Sho},
	journal = {Perspectives on Psychological Science},
	number = {4},
	pages = {1026--1041},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Transparency and reproducibility of meta-analyses in psychology: A meta-review},
	volume = {15},
	year = {2020}}

@article{nust2018reproducible,
	author = {N{\"u}st, Daniel and Granell, Carlos and Hofer, Barbara and Konkol, Markus and Ostermann, Frank O and Sileryte, Rusne and Cerutti, Valentina},
	journal = {PeerJ},
	pages = {e5072},
	publisher = {PeerJ Inc.},
	title = {Reproducible research and GIScience: an evaluation using AGILE conference papers},
	volume = {6},
	year = {2018}}

@article{Wang2015,
	abstract = {Context: The scientific community and decision-makers are increasingly concerned about transparency and reproducibility of epidemiologic studies using longitudinal healthcare databases. We explored the extent to which published pharmacoepidemiologic studies using commercially available databases could be reproduced by other investigators. We identified a nonsystematic sample of 38 descriptive or comparative safety/effectiveness cohort studies. Seven studies were excluded from reproduction, five because of violation of fundamental design principles, and two because of grossly inadequate reporting. In the remaining studies, >1,000 patient characteristics and measures of association were reproduced with a high degree of accuracy (median differences between original and reproduction <2\% and <0.1). An essential component of transparent and reproducible research with healthcare databases is more complete reporting of study implementation. Once reproducibility is achieved, the conversation can be elevated to assess whether suboptimal design choices led to avoidable bias and whether findings are replicable in other data sources.},
	author = {SV Wang, P Verpillat, JA Rassen, A Patrick, EM Garry and DB Bartels},
	doi = {https://doi.org/10.1002/cpt329.infsof.2016.03.003},
	issn = {0950-5849},
	journal = {CLINICAL PHARMACOLOGY & THERAPEUTICS},
	keywords = {Replication, Reproducibility, Software defect prediction},
	pages = {325-332},
	title = {Transparency and reproducibility of observational cohort studies using large healthcare databases},
	url = {https://pubmed.ncbi.nlm.nih.gov/26690726/},
	volume = {99},
	year = {2015},
	bdsk-url-1 = {https://pubmed.ncbi.nlm.nih.gov/26690726/},
	bdsk-url-2 = {https://doi.org/10.1002/cpt329.infsof.2016.03.003}}

@article{MAHMOOD2018148,
	abstract = {Context: Replications are an important part of scientific disciplines. Replications test the credibility of original studies and can separate true results from those that are unreliable. Objective: In this paper we investigate the replication of defect prediction studies and identify the characteristics of replicated studies. We further assess how defect prediction replications are performed and the consistency of replication findings. Method: Our analysis is based on tracking the replication of 208 defect prediction studies identified by a highly cited Systematic Literature Review (SLR) [1]. We identify how often each of these 208 studies has been replicated and determine the type of replication carried out. We identify quality, citation counts, publication venue, impact factor, and data availability from all 208 SLR defect prediction papers to see if any of these factors are associated with the frequency with which they are replicated. Results: Only 13 (6\%) of the 208 studies are replicated. Replication seems related to original papers appearing in the Transactions of Software Engineering (TSE) journal. The number of citations an original paper had was also an indicator of replications. In addition, studies conducted using closed source data seems to have more replications than those based on open source data. Where a paper has been replicated, 11 (38\%) out of 29 studies revealed different results to the original study. Conclusion: Very few defect prediction studies are replicated. The lack of replication means that it remains unclear how reliable defect prediction is. We provide practical steps for improving the state of replication.},
	author = {Zaheed Mahmood and David Bowes and Tracy Hall and Peter C.R. Lane and Jean Petri{\'c}},
	doi = {https://doi.org/10.1016/j.infsof.2018.02.003},
	issn = {0950-5849},
	journal = {Information and Software Technology},
	keywords = {Replication, Reproducibility, Software defect prediction},
	pages = {148-163},
	title = {Reproducibility and replicability of software defect prediction studies},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584917304202},
	volume = {99},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950584917304202},
	bdsk-url-2 = {https://doi.org/10.1016/j.infsof.2018.02.003}}

@article{PAGE20188,
	abstract = {Objectives
To evaluate how often reproducible research practices, which allow others to recreate the findings of studies, given the original data, are used in systematic reviews (SRs) of biomedical research.
Study Design and Setting
We evaluated a random sample of SRs indexed in MEDLINE during February 2014, which focused on a therapeutic intervention and reported at least one meta-analysis. Data on reproducible research practices in each SR were extracted using a 26-item form by one author, with a 20\% random sample extracted in duplicate. We explored whether the use of reproducible research practices was associated with an SR being a Cochrane review, as well as with the reported use of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses statement.
Results
We evaluated 110 SRs of therapeutic interventions, 78 (71\%) of which were non-Cochrane SRs. Across the SRs, there were 2,139 meta-analytic effects (including subgroup meta-analytic effects and sensitivity analyses), 1,551 (73\%) of which were reported in sufficient detail to recreate them. Systematic reviewers reported the data needed to recreate all meta-analytic effects in 72 (65\%) SRs only. This percentage was higher in Cochrane than in non-Cochrane SRs (30/32 [94\%] vs. 42/78 [54\%]; risk ratio 1.74, 95\% confidence interval 1.39--2.18). Systematic reviewers who reported imputing, algebraically manipulating, or obtaining some data from the study author/sponsor infrequently stated which specific data were handled in this way. Only 33 (30\%) SRs mentioned access to data sets and statistical code used to perform analyses.
Conclusion
Reproducible research practices are underused in SRs of biomedical interventions. Adoption of such practices facilitates identification of errors and allows the SR data to be reanalyzed.},
	author = {Matthew J. Page and Douglas G. Altman and Larissa Shamseer and Joanne E. McKenzie and Nadera Ahmadzai and Dianna Wolfe and Fatemeh Yazdi and Ferr{\'a}n Catal{\'a}-L{\'o}pez and Andrea C. Tricco and David Moher},
	doi = {https://doi.org/10.1016/j.jclinepi.2017.10.017},
	issn = {0895-4356},
	journal = {Journal of Clinical Epidemiology},
	keywords = {Reproducibility, Reporting, Systematic reviews, Methodology, Quality, Data sharing},
	pages = {8-18},
	title = {Reproducible research practices are underused in systematic reviews of biomedical interventions},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435617305358},
	volume = {94},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0895435617305358},
	bdsk-url-2 = {https://doi.org/10.1016/j.jclinepi.2017.10.017}}

@article{Gundersen2019,
	author = {Gundersen, Odd Erik},
	doi = {10.1609/aimag.v40i4.5185},
	journal = {AI Magazine},
	month = {12},
	pages = {9-23},
	title = {Standing on the Feet of Giants --- Reproducibility in AI},
	volume = {40},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1609/aimag.v40i4.5185}}

@article{Wu2021PredictingTR,
	author = {Jian Wu and Rajal Nivargi and Sree Sai Teja Lanka and Arjun Manoj Menon and Sai Ajay Modukuri and Nishanth Nakshatri and Xin Wei and Zhuoer Wang and James Caverlee and Sarah Michele Rajtmajer and C. Lee Giles},
	journal = {ArXiv},
	title = {Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Models},
	volume = {abs/2104.04580},
	year = {2021}}
